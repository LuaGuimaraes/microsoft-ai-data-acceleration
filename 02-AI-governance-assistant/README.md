# ğŸ¤– AI Governance Assistant  

### Uso consciente, rastreÃ¡vel e auditÃ¡vel de InteligÃªncia Artificial
Este projeto faz parte da **AceleraÃ§Ã£o Microsoft â€“ IA e Arquitetura de Dados (DIO)** e tem como objetivo demonstrar, na prÃ¡tica, **como implementar governanÃ§a, auditoria e observabilidade no uso de modelos de linguagem (LLMs)** em ambientes corporativos.

Ao invÃ©s de focar apenas em â€œbloquear respostasâ€, este projeto explora **como monitorar o comportamento de uso da IA**, identificar riscos e manter rastreabilidade completa para fins de compliance, auditoria e seguranÃ§a.
---

## ğŸ§  Contexto do Problema
Em ambientes empresariais e governamentais, o risco nÃ£o estÃ¡ apenas **no que a IA responde**, mas **em como os usuÃ¡rios interagem com ela ao longo do tempo**.

Perguntas reformuladas, tentativas repetidas, mudanÃ§as sutis de contexto e engenharia de prompt podem permitir que conteÃºdos sensÃ­veis ou potencialmente ilegais â€œpassemâ€ pelos filtros tradicionais de seguranÃ§a.

Este projeto demonstra como **a governanÃ§a nÃ£o deve depender apenas do modelo**, mas sim de uma **arquitetura de observabilidade e auditoria**.
---

## ğŸ¯ Objetivos do Projeto
- Demonstrar **governanÃ§a aplicada ao uso de LLMs**
- Implementar **auditoria de prompts e respostas**
- Detectar **padrÃµes de uso sensÃ­veis ou de risco**
- Manter **rastreamento completo (traceability)**
- Diferenciar **content safety** de **uso indevido**
- Simular cenÃ¡rios reais de ambiente corporativo
---

## ğŸ—ï¸ Arquitetura e Componentes

### ğŸ”¹ Interface
- **Streamlit** para interaÃ§Ã£o com o usuÃ¡rio

### ğŸ”¹ Modelo de Linguagem
- **Google Gemini**
- Utilizado como alternativa ao OpenAI por limitaÃ§Ãµes financeiras comuns a estudantes  
- A escolha do modelo **nÃ£o impacta os princÃ­pios de governanÃ§a demonstrados**

### ğŸ”¹ GovernanÃ§a e Observabilidade
- **Langfuse**
  - Tracing de prompts e respostas
  - Registro de tentativas bloqueadas e permitidas
  - Auditoria de comportamento ao longo do tempo
  - ClassificaÃ§Ã£o de risco baseada no uso, nÃ£o apenas no conteÃºdo

### ğŸ”¹ SeguranÃ§a de ConteÃºdo
- Regras de compliance aplicadas antes e depois da chamada ao modelo
- Bloqueio explÃ­cito de perguntas diretas ilegais
- Registro de tentativas de â€œprompt refactoringâ€
---

## ğŸ” O Diferencial: GovernanÃ§a Baseada em Uso
Mesmo quando uma pergunta **passa pelos filtros do modelo** por estar formulada de forma educacional ou acadÃªmica, o sistema:

- MantÃ©m o histÃ³rico completo da interaÃ§Ã£o
- Registra tentativas anteriores bloqueadas
- Permite auditoria posterior
- Evidencia padrÃµes de uso sensÃ­vel

Isso reflete prÃ¡ticas reais de:
- **Auditoria corporativa**
- **Insider risk management**
- **Compliance e governanÃ§a de IA**
---

## ğŸ”§ DecisÃ£o TÃ©cnica: Uso do Gemini
Devido a limitaÃ§Ãµes financeiras comuns a estudantes, foi necessÃ¡rio **refatorar o cÃ³digo originalmente pensado para OpenAI**, adaptando-o para uso com **Google Gemini**.

Essa adaptaÃ§Ã£o foi feita de forma consciente, utilizando:
- Engenharia de prompt
- Apoio de IA como ferramenta auxiliar de desenvolvimento
- AvaliaÃ§Ã£o crÃ­tica das diferenÃ§as entre APIs

Essa decisÃ£o **nÃ£o compromete o objetivo do projeto**, que Ã© demonstrar governanÃ§a, auditoria e rastreabilidade, independentemente do fornecedor do modelo.
---

## âš ï¸ LimitaÃ§Ãµes Conhecidas
- Projeto educacional, nÃ£o implantado em ambiente produtivo real
- Regras de risco simplificadas para fins didÃ¡ticos
- ClassificaÃ§Ã£o de risco baseada em heurÃ­sticas, nÃ£o em modelos dedicados

Essas limitaÃ§Ãµes sÃ£o **intencionais** e fazem parte do escopo educacional.
---

## ğŸš€ PrÃ³ximos Passos
- IntegraÃ§Ã£o com **Azure AI Content Safety**
- PontuaÃ§Ã£o automÃ¡tica de risco por sessÃ£o
- Dashboards executivos de uso e compliance
- IntegraÃ§Ã£o com pipelines de MLflow
- SimulaÃ§Ã£o de polÃ­ticas internas por perfil de usuÃ¡rio
---

## ğŸ“œ Aviso Ã‰tico
Este projeto tem **finalidade exclusivamente educacional**.  
Nenhuma informaÃ§Ã£o apresentada deve ser interpretada como incentivo, instruÃ§Ã£o ou apoio a prÃ¡ticas ilegais.

O foco Ã© **prevenÃ§Ã£o, auditoria e governanÃ§a** no uso responsÃ¡vel de InteligÃªncia Artificial.
---

Desenvolvido por **Luana GuimarÃ£es**  